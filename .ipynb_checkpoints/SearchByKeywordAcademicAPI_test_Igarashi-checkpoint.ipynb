{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from file (emotion keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "wordlist_df = pd.read_excel('../data/WordList.xlsx')\n",
    "wordlist = wordlist_df[\"orignal form\"].to_list() #store original words in df\n",
    "\n",
    "search_list = wordlist_df[~wordlist_df['Unnamed: 4'].isnull()]['Unnamed: 4'].to_list()\n",
    "search_list_cat = wordlist_df[~wordlist_df['Unnamed: 4'].isnull()]['file name'].to_list()\n",
    "\n",
    "def QueryWordsReg(searchwords):\n",
    "    return re.sub('%20',' ', searchwords) +  ' -is:reply -is:retweet lang:ja'\n",
    "\n",
    "querylist = [QueryWordsReg(searchwords) for searchwords in search_list ]\n",
    "\n",
    "#      modify query words in case of changes in API v2 \n",
    "#\n",
    "#      eg. 'å¿ƒé…ã§ã™%20OR%20æ‡¸å¿µ%20OR%20è½ã¡ç€ã‹ãªã„%20OR%20ç·Šå¼µã™ã‚‹%20OR%20å¿ƒé…ã %20OR%20ä¸å®‰ã§ã™%20OR%20ä¸å®‰ã '\n",
    "#     \n",
    "#     ----->>>    'å¿ƒé…ã§ã™ OR æ‡¸å¿µ OR è½ã¡ç€ã‹ãªã„ OR ç·Šå¼µã™ã‚‹ OR å¿ƒé…ã  OR ä¸å®‰ã§ã™ OR ä¸å®‰ã  lang:ja'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "CK = '' # Consumer Key\n",
    "CS = '' # Consumer Secret\n",
    "AT = '' # Access Token\n",
    "AS = '' # Accesss Token Secert\n",
    "\n",
    "\n",
    "BT = ''\n",
    "# Bearer Token\n",
    "\n",
    "\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "#keyword = querylist[2] #query words for test\n",
    "keyword = \"ãƒ†ãƒ‹ã‚¹\" #query words for test\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "\n",
    "query_params = {'query': keyword ,\n",
    "                'tweet.fields': 'created_at',\n",
    "                #'expansions': 'author_id',\n",
    "                'start_time': '2021-02-15T00:00:00Z',\n",
    "                'end_time': '2021-02-15T09:00:00Z',\n",
    "                #'user.fields': 'description', # profile information of author\n",
    "                'max_results':500,  #ä¸€å›ã®queryã¯ï¼•ï¼ï¼ã§ä¸Šé™ã‚‰ã—ã„\n",
    "                'next_token' : {} #æ¬¡ã®ãƒšãƒ¼ã‚¸ã«ã„ããŸã‚ã®paramï¼Ÿ\n",
    "               }\n",
    "\n",
    "\n",
    "def create_headers(BT):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(BT)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params):\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "    #print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "#    300 requests per 15-minute window (app auth)\n",
    "#\n",
    "#    1 request per second (app auth)\n",
    "#\n",
    "#    ----->>> sleep 1 sec between every query\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    count = 0\n",
    "    flag = True\n",
    "    TWEET_LIMIT = 1000000    \n",
    "    cols = ['created_at', 'id', ' text']#[ã“ã“æ”¹è‰¯]\n",
    "    normalized_data_old = pd.DataFrame(index=[], columns=cols)#[ã“ã“æ”¹è‰¯]\n",
    "    \n",
    "    while flag:\n",
    "        #print('Collected Tweets: %.2f%% \\r' %(count*100/TWEET_LIMIT), end = '')\n",
    "        if count >= TWEET_LIMIT:\n",
    "            break\n",
    "    \n",
    "        headers = create_headers(BT)\n",
    "        time.sleep(1)\n",
    "        json_response = connect_to_endpoint(search_url, headers, query_params)\n",
    "        normalized_data = pd.json_normalize(json_response['data'])#[ã“ã“æ”¹è‰¯]\n",
    "        normalized_data_new = pd.concat([normalized_dataï¼¿old, normalized_data])#[ã“ã“æ”¹è‰¯]\n",
    "        normalized_dataï¼¿old=normalized_data_new#[ã“ã“æ”¹è‰¯]\n",
    "        print(\"total:\"+str(len(normalized_dataï¼¿old))+\"tweets\")\n",
    "        #print(json_response)\n",
    "        #print(len(json_response['data']))\n",
    "    \n",
    "        result_count = json_response['meta']['result_count']\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            \n",
    "            query_params['next_token'] = next_token\n",
    "            \n",
    "            count += result_count\n",
    "            \n",
    "            time.sleep(3)  # rate limit = 1 request/1 sec\n",
    "            \n",
    "            json_response = connect_to_endpoint(search_url, headers, query_params)\n",
    "            \n",
    "            normalized_data = pd.json_normalize(json_response['data'])#[ã“ã“æ”¹è‰¯]\n",
    "            normalized_data_new = pd.concat([normalized_dataï¼¿old, normalized_data])#[ã“ã“æ”¹è‰¯]\n",
    "            normalized_dataï¼¿old=normalized_data_new#[ã“ã“æ”¹è‰¯]\n",
    "            print(\"total:\"+str(len(normalized_dataï¼¿old))+\"tweets\")\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "        else:\n",
    "            flag = False\n",
    "            \n",
    "    normalized_data_last=normalized_data_old#[ã“ã“æ”¹è‰¯]\n",
    "    #normalized_data = pd.json_normalize(json_response['data'])\n",
    "    #normalized_users = pd.json_normalize(json_response['includes']['users']).set_index('id')\n",
    "    #normalized_places = pd.json_normalize(json_response['includes']['places']).set_index('id')\n",
    "    #print(len(normalized_data_last))#[ã“ã“æ”¹è‰¯]\n",
    "            \n",
    "    #normalized_users.rename({'id': 'author_id'}, inplace = True, axis = 1)\n",
    "    #normalized_data.rename({'geo.place_id': 'place_id'}, inplace = True, axis = 1)\n",
    "    #normalized_places.rename({'id': 'place_id'}, inplace = True, axis = 1)\n",
    "            \n",
    "    #normalized = normalized_data.join(normalized_users, on = 'author_id', how = 'outer', rsuffix = '_user')\n",
    "    #normalized = normalized.join(normalized_places, on = 'place_id', how = 'outer', rsuffix = '_place')\n",
    "\n",
    "    normalized_data_last.to_csv('SearchTestAcademicAPI.csv')  #[ã“ã“æ”¹è‰¯]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:452tweets\n",
      "total:915tweets\n",
      "total:1378tweets\n",
      "total:1840tweets\n",
      "total:2302tweets\n",
      "total:2776tweets\n",
      "total:3250tweets\n",
      "total:3722tweets\n",
      "total:4194tweets\n",
      "total:4667tweets\n",
      "total:5140tweets\n",
      "total:5607tweets\n",
      "total:6074tweets\n",
      "total:6498tweets\n",
      "total:6922tweets\n",
      "total:7381tweets\n",
      "total:7840tweets\n",
      "total:7951tweets\n",
      "total:8062tweets\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('SearchTestAcademicAPI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-15T08:59:58.000Z</td>\n",
       "      <td>1361238812727205897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ğŸŒ·äº¤æ›ğŸŒ·\\næ–°ãƒ†ãƒ‹ã‚¹ã®ç‹å­æ§˜ã€€ã‚¢ãƒ‹ãƒ¡ã‚¤ãƒˆç‰¹å…¸ã€€ãƒã‚±ãƒƒãƒˆé¢¨ã‚«ãƒ¼ãƒ‰\\n\\nè­²â†’ç”»åƒã®ã‚‚ã®\\næ±‚â†’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-02-15T08:59:56.000Z</td>\n",
       "      <td>1361238806276362241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @nhk_chubu: ã‚ã™16æ—¥(ç«)ã® #ã•ã‚‰ã•ã‚‰ã‚µãƒ©ãƒ€ ã¯\\nå…¨è±ªã‚ªãƒ¼ãƒ—ãƒ³ãƒ†ãƒ‹ã‚¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-15T08:59:51.000Z</td>\n",
       "      <td>1361238785002905603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@namseokdiary ã ã‚ˆã­wwwè¿·ã‚ãšä¼šå“¡ã«ãªã‚‹ğŸ˜­\\nãƒ†ãƒ‹ã‚¹æ•™å®¤ã®ãŸã‚ã«åƒãã¨è¨€ã£ã¦...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-02-15T08:59:50.000Z</td>\n",
       "      <td>1361238782192754694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ã„ã¤ã§ã‚‚åœ°å‘³ãªãƒ†ãƒ‹ã‚¹éƒ¨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-02-15T08:59:46.000Z</td>\n",
       "      <td>1361238762752077827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@tae_kumachan ã¡ã‚‡ã“ã¡ã‚ƒã‚“ãƒ†ãƒ‹ã‚¹éƒ¨ã ã£ãŸã‚“ã ã­ğŸ¾\\nç§ã¯å¸°å®…éƒ¨ğŸ¡ç¬‘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8057</th>\n",
       "      <td>106</td>\n",
       "      <td>2021-02-15T00:00:59.000Z</td>\n",
       "      <td>1361103172618448896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ã€æ‹¡æ•£å¸Œæœ›ã€‘è¶Šå‰ ãƒªãƒ§ãƒ¼ãƒ ã‚°ãƒƒã‚º ã‚’åé›†ã—ã¦ã„ã¾ã™ã€‚ãƒ©ãƒã‚¹ãƒˆã‚„ç¼¶ãƒãƒƒãƒãªã©èº«ã«ä»˜ã‘ã‚‰ã‚Œã‚‹ã‚°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8058</th>\n",
       "      <td>107</td>\n",
       "      <td>2021-02-15T00:00:49.000Z</td>\n",
       "      <td>1361103133045362688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ã‚·ãƒ§ãƒƒã‚­ãƒ³ã‚°ãªåˆæˆ¦æ•—é€€ã§\\næ¶™ã‚’è¦‹ã›ãŸãƒ¢ãƒ³ãƒ•ã‚£ã‚¹ğŸ‡«ğŸ‡·\\n\\nã€Œåƒ•ã¯å¿…ãšå¾©æ´»ã™ã‚‹è‡ªä¿¡ãŒã‚ã‚‹ã€‚åƒ•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8059</th>\n",
       "      <td>108</td>\n",
       "      <td>2021-02-15T00:00:26.000Z</td>\n",
       "      <td>1361103035695394817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ãƒ†ãƒ‹ã‚¹ã‚³ãƒ¼ãƒˆåºƒã„ã—ã‚­ãƒ¬ã‚¤ã¨ä»–æ ¡ã®å­ã‹ã‚‰è¨€ã‚ã‚ŒãŸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8060</th>\n",
       "      <td>109</td>\n",
       "      <td>2021-02-15T00:00:16.000Z</td>\n",
       "      <td>1361102995727912961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @hochi_hanbai: #ãƒ†ãƒ‹ã‚¹ ã® #å…¨è±ªã‚ªãƒ¼ãƒ—ãƒ³ å¥³å­ã‚·ãƒ³ã‚°ãƒ«ã‚¹ç¬¬3ã‚·ãƒ¼ãƒ‰ã§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8061</th>\n",
       "      <td>110</td>\n",
       "      <td>2021-02-15T00:00:16.000Z</td>\n",
       "      <td>1361102992586526721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>â†’ãã†ã„ã†äººãŒã€ã©ã†ã‚„ã£ãŸã‚‰å‰å‘ãã«è©¦åˆã«å‘ãåˆãˆã‚‹ã‹ãªã€‚ã€‚èª°ã‹ã€ä¹—ã‚Šè¶ŠãˆãŸçµŒé¨“ã¨ã‹ã€ã‚¢ãƒ‰ãƒ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8062 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                created_at                   id   text  \\\n",
       "0              0  2021-02-15T08:59:58.000Z  1361238812727205897    NaN   \n",
       "1              1  2021-02-15T08:59:56.000Z  1361238806276362241    NaN   \n",
       "2              2  2021-02-15T08:59:51.000Z  1361238785002905603    NaN   \n",
       "3              3  2021-02-15T08:59:50.000Z  1361238782192754694    NaN   \n",
       "4              4  2021-02-15T08:59:46.000Z  1361238762752077827    NaN   \n",
       "...          ...                       ...                  ...    ...   \n",
       "8057         106  2021-02-15T00:00:59.000Z  1361103172618448896    NaN   \n",
       "8058         107  2021-02-15T00:00:49.000Z  1361103133045362688    NaN   \n",
       "8059         108  2021-02-15T00:00:26.000Z  1361103035695394817    NaN   \n",
       "8060         109  2021-02-15T00:00:16.000Z  1361102995727912961    NaN   \n",
       "8061         110  2021-02-15T00:00:16.000Z  1361102992586526721    NaN   \n",
       "\n",
       "                                                   text  \n",
       "0     ğŸŒ·äº¤æ›ğŸŒ·\\næ–°ãƒ†ãƒ‹ã‚¹ã®ç‹å­æ§˜ã€€ã‚¢ãƒ‹ãƒ¡ã‚¤ãƒˆç‰¹å…¸ã€€ãƒã‚±ãƒƒãƒˆé¢¨ã‚«ãƒ¼ãƒ‰\\n\\nè­²â†’ç”»åƒã®ã‚‚ã®\\næ±‚â†’...  \n",
       "1     RT @nhk_chubu: ã‚ã™16æ—¥(ç«)ã® #ã•ã‚‰ã•ã‚‰ã‚µãƒ©ãƒ€ ã¯\\nå…¨è±ªã‚ªãƒ¼ãƒ—ãƒ³ãƒ†ãƒ‹ã‚¹...  \n",
       "2     @namseokdiary ã ã‚ˆã­wwwè¿·ã‚ãšä¼šå“¡ã«ãªã‚‹ğŸ˜­\\nãƒ†ãƒ‹ã‚¹æ•™å®¤ã®ãŸã‚ã«åƒãã¨è¨€ã£ã¦...  \n",
       "3                                           ã„ã¤ã§ã‚‚åœ°å‘³ãªãƒ†ãƒ‹ã‚¹éƒ¨  \n",
       "4              @tae_kumachan ã¡ã‚‡ã“ã¡ã‚ƒã‚“ãƒ†ãƒ‹ã‚¹éƒ¨ã ã£ãŸã‚“ã ã­ğŸ¾\\nç§ã¯å¸°å®…éƒ¨ğŸ¡ç¬‘  \n",
       "...                                                 ...  \n",
       "8057  ã€æ‹¡æ•£å¸Œæœ›ã€‘è¶Šå‰ ãƒªãƒ§ãƒ¼ãƒ ã‚°ãƒƒã‚º ã‚’åé›†ã—ã¦ã„ã¾ã™ã€‚ãƒ©ãƒã‚¹ãƒˆã‚„ç¼¶ãƒãƒƒãƒãªã©èº«ã«ä»˜ã‘ã‚‰ã‚Œã‚‹ã‚°...  \n",
       "8058  ã‚·ãƒ§ãƒƒã‚­ãƒ³ã‚°ãªåˆæˆ¦æ•—é€€ã§\\næ¶™ã‚’è¦‹ã›ãŸãƒ¢ãƒ³ãƒ•ã‚£ã‚¹ğŸ‡«ğŸ‡·\\n\\nã€Œåƒ•ã¯å¿…ãšå¾©æ´»ã™ã‚‹è‡ªä¿¡ãŒã‚ã‚‹ã€‚åƒ•...  \n",
       "8059                            ãƒ†ãƒ‹ã‚¹ã‚³ãƒ¼ãƒˆåºƒã„ã—ã‚­ãƒ¬ã‚¤ã¨ä»–æ ¡ã®å­ã‹ã‚‰è¨€ã‚ã‚ŒãŸ  \n",
       "8060  RT @hochi_hanbai: #ãƒ†ãƒ‹ã‚¹ ã® #å…¨è±ªã‚ªãƒ¼ãƒ—ãƒ³ å¥³å­ã‚·ãƒ³ã‚°ãƒ«ã‚¹ç¬¬3ã‚·ãƒ¼ãƒ‰ã§...  \n",
       "8061  â†’ãã†ã„ã†äººãŒã€ã©ã†ã‚„ã£ãŸã‚‰å‰å‘ãã«è©¦åˆã«å‘ãåˆãˆã‚‹ã‹ãªã€‚ã€‚èª°ã‹ã€ä¹—ã‚Šè¶ŠãˆãŸçµŒé¨“ã¨ã‹ã€ã‚¢ãƒ‰ãƒ...  \n",
       "\n",
       "[8062 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
