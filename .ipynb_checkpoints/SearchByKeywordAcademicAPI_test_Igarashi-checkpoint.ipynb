{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from file (emotion keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "wordlist_df = pd.read_excel('../data/WordList.xlsx')\n",
    "wordlist = wordlist_df[\"orignal form\"].to_list() #store original words in df\n",
    "\n",
    "search_list = wordlist_df[~wordlist_df['Unnamed: 4'].isnull()]['Unnamed: 4'].to_list()\n",
    "search_list_cat = wordlist_df[~wordlist_df['Unnamed: 4'].isnull()]['file name'].to_list()\n",
    "\n",
    "def QueryWordsReg(searchwords):\n",
    "    return re.sub('%20',' ', searchwords) +  ' -is:reply -is:retweet lang:ja'\n",
    "\n",
    "querylist = [QueryWordsReg(searchwords) for searchwords in search_list ]\n",
    "\n",
    "#      modify query words in case of changes in API v2 \n",
    "#\n",
    "#      eg. '心配です%20OR%20懸念%20OR%20落ち着かない%20OR%20緊張する%20OR%20心配だ%20OR%20不安です%20OR%20不安だ'\n",
    "#     \n",
    "#     ----->>>    '心配です OR 懸念 OR 落ち着かない OR 緊張する OR 心配だ OR 不安です OR 不安だ lang:ja'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "CK = '' # Consumer Key\n",
    "CS = '' # Consumer Secret\n",
    "AT = '' # Access Token\n",
    "AS = '' # Accesss Token Secert\n",
    "\n",
    "\n",
    "BT = ''\n",
    "# Bearer Token\n",
    "\n",
    "\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "#keyword = querylist[2] #query words for test\n",
    "keyword = \"テニス\" #query words for test\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "\n",
    "query_params = {'query': keyword ,\n",
    "                'tweet.fields': 'created_at',\n",
    "                #'expansions': 'author_id',\n",
    "                'start_time': '2021-02-15T00:00:00Z',\n",
    "                'end_time': '2021-02-15T09:00:00Z',\n",
    "                #'user.fields': 'description', # profile information of author\n",
    "                'max_results':500,  #一回のqueryは５００で上限らしい\n",
    "                'next_token' : {} #次のページにいくためのparam？\n",
    "               }\n",
    "\n",
    "\n",
    "def create_headers(BT):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(BT)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params):\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "    #print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "#    300 requests per 15-minute window (app auth)\n",
    "#\n",
    "#    1 request per second (app auth)\n",
    "#\n",
    "#    ----->>> sleep 1 sec between every query\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    count = 0\n",
    "    flag = True\n",
    "    TWEET_LIMIT = 1000000    \n",
    "    cols = ['created_at', 'id', ' text']#[ここ改良]\n",
    "    normalized_data_old = pd.DataFrame(index=[], columns=cols)#[ここ改良]\n",
    "    \n",
    "    while flag:\n",
    "        #print('Collected Tweets: %.2f%% \\r' %(count*100/TWEET_LIMIT), end = '')\n",
    "        if count >= TWEET_LIMIT:\n",
    "            break\n",
    "    \n",
    "        headers = create_headers(BT)\n",
    "        time.sleep(1)\n",
    "        json_response = connect_to_endpoint(search_url, headers, query_params)\n",
    "        normalized_data = pd.json_normalize(json_response['data'])#[ここ改良]\n",
    "        normalized_data_new = pd.concat([normalized_data＿old, normalized_data])#[ここ改良]\n",
    "        normalized_data＿old=normalized_data_new#[ここ改良]\n",
    "        print(\"total:\"+str(len(normalized_data＿old))+\"tweets\")\n",
    "        #print(json_response)\n",
    "        #print(len(json_response['data']))\n",
    "    \n",
    "        result_count = json_response['meta']['result_count']\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            \n",
    "            query_params['next_token'] = next_token\n",
    "            \n",
    "            count += result_count\n",
    "            \n",
    "            time.sleep(3)  # rate limit = 1 request/1 sec\n",
    "            \n",
    "            json_response = connect_to_endpoint(search_url, headers, query_params)\n",
    "            \n",
    "            normalized_data = pd.json_normalize(json_response['data'])#[ここ改良]\n",
    "            normalized_data_new = pd.concat([normalized_data＿old, normalized_data])#[ここ改良]\n",
    "            normalized_data＿old=normalized_data_new#[ここ改良]\n",
    "            print(\"total:\"+str(len(normalized_data＿old))+\"tweets\")\n",
    "            #import pdb; pdb.set_trace()\n",
    "\n",
    "        else:\n",
    "            flag = False\n",
    "            \n",
    "    normalized_data_last=normalized_data_old#[ここ改良]\n",
    "    #normalized_data = pd.json_normalize(json_response['data'])\n",
    "    #normalized_users = pd.json_normalize(json_response['includes']['users']).set_index('id')\n",
    "    #normalized_places = pd.json_normalize(json_response['includes']['places']).set_index('id')\n",
    "    #print(len(normalized_data_last))#[ここ改良]\n",
    "            \n",
    "    #normalized_users.rename({'id': 'author_id'}, inplace = True, axis = 1)\n",
    "    #normalized_data.rename({'geo.place_id': 'place_id'}, inplace = True, axis = 1)\n",
    "    #normalized_places.rename({'id': 'place_id'}, inplace = True, axis = 1)\n",
    "            \n",
    "    #normalized = normalized_data.join(normalized_users, on = 'author_id', how = 'outer', rsuffix = '_user')\n",
    "    #normalized = normalized.join(normalized_places, on = 'place_id', how = 'outer', rsuffix = '_place')\n",
    "\n",
    "    normalized_data_last.to_csv('SearchTestAcademicAPI.csv')  #[ここ改良]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:452tweets\n",
      "total:915tweets\n",
      "total:1378tweets\n",
      "total:1840tweets\n",
      "total:2302tweets\n",
      "total:2776tweets\n",
      "total:3250tweets\n",
      "total:3722tweets\n",
      "total:4194tweets\n",
      "total:4667tweets\n",
      "total:5140tweets\n",
      "total:5607tweets\n",
      "total:6074tweets\n",
      "total:6498tweets\n",
      "total:6922tweets\n",
      "total:7381tweets\n",
      "total:7840tweets\n",
      "total:7951tweets\n",
      "total:8062tweets\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('SearchTestAcademicAPI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-15T08:59:58.000Z</td>\n",
       "      <td>1361238812727205897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>🌷交換🌷\\n新テニスの王子様　アニメイト特典　チケット風カード\\n\\n譲→画像のもの\\n求→...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-02-15T08:59:56.000Z</td>\n",
       "      <td>1361238806276362241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @nhk_chubu: あす16日(火)の #さらさらサラダ は\\n全豪オープンテニス...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-15T08:59:51.000Z</td>\n",
       "      <td>1361238785002905603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@namseokdiary だよねwww迷わず会員になる😭\\nテニス教室のために働くと言って...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-02-15T08:59:50.000Z</td>\n",
       "      <td>1361238782192754694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>いつでも地味なテニス部</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-02-15T08:59:46.000Z</td>\n",
       "      <td>1361238762752077827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@tae_kumachan ちょこちゃんテニス部だったんだね🎾\\n私は帰宅部🏡笑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8057</th>\n",
       "      <td>106</td>\n",
       "      <td>2021-02-15T00:00:59.000Z</td>\n",
       "      <td>1361103172618448896</td>\n",
       "      <td>NaN</td>\n",
       "      <td>【拡散希望】越前 リョーマ グッズ を収集しています。ラバストや缶バッチなど身に付けられるグ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8058</th>\n",
       "      <td>107</td>\n",
       "      <td>2021-02-15T00:00:49.000Z</td>\n",
       "      <td>1361103133045362688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ショッキングな初戦敗退で\\n涙を見せたモンフィス🇫🇷\\n\\n「僕は必ず復活する自信がある。僕...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8059</th>\n",
       "      <td>108</td>\n",
       "      <td>2021-02-15T00:00:26.000Z</td>\n",
       "      <td>1361103035695394817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>テニスコート広いしキレイと他校の子から言われた</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8060</th>\n",
       "      <td>109</td>\n",
       "      <td>2021-02-15T00:00:16.000Z</td>\n",
       "      <td>1361102995727912961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @hochi_hanbai: #テニス の #全豪オープン 女子シングルス第3シードで...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8061</th>\n",
       "      <td>110</td>\n",
       "      <td>2021-02-15T00:00:16.000Z</td>\n",
       "      <td>1361102992586526721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>→そういう人が、どうやったら前向きに試合に向き合えるかな。。誰か、乗り越えた経験とか、アドバ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8062 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                created_at                   id   text  \\\n",
       "0              0  2021-02-15T08:59:58.000Z  1361238812727205897    NaN   \n",
       "1              1  2021-02-15T08:59:56.000Z  1361238806276362241    NaN   \n",
       "2              2  2021-02-15T08:59:51.000Z  1361238785002905603    NaN   \n",
       "3              3  2021-02-15T08:59:50.000Z  1361238782192754694    NaN   \n",
       "4              4  2021-02-15T08:59:46.000Z  1361238762752077827    NaN   \n",
       "...          ...                       ...                  ...    ...   \n",
       "8057         106  2021-02-15T00:00:59.000Z  1361103172618448896    NaN   \n",
       "8058         107  2021-02-15T00:00:49.000Z  1361103133045362688    NaN   \n",
       "8059         108  2021-02-15T00:00:26.000Z  1361103035695394817    NaN   \n",
       "8060         109  2021-02-15T00:00:16.000Z  1361102995727912961    NaN   \n",
       "8061         110  2021-02-15T00:00:16.000Z  1361102992586526721    NaN   \n",
       "\n",
       "                                                   text  \n",
       "0     🌷交換🌷\\n新テニスの王子様　アニメイト特典　チケット風カード\\n\\n譲→画像のもの\\n求→...  \n",
       "1     RT @nhk_chubu: あす16日(火)の #さらさらサラダ は\\n全豪オープンテニス...  \n",
       "2     @namseokdiary だよねwww迷わず会員になる😭\\nテニス教室のために働くと言って...  \n",
       "3                                           いつでも地味なテニス部  \n",
       "4              @tae_kumachan ちょこちゃんテニス部だったんだね🎾\\n私は帰宅部🏡笑  \n",
       "...                                                 ...  \n",
       "8057  【拡散希望】越前 リョーマ グッズ を収集しています。ラバストや缶バッチなど身に付けられるグ...  \n",
       "8058  ショッキングな初戦敗退で\\n涙を見せたモンフィス🇫🇷\\n\\n「僕は必ず復活する自信がある。僕...  \n",
       "8059                            テニスコート広いしキレイと他校の子から言われた  \n",
       "8060  RT @hochi_hanbai: #テニス の #全豪オープン 女子シングルス第3シードで...  \n",
       "8061  →そういう人が、どうやったら前向きに試合に向き合えるかな。。誰か、乗り越えた経験とか、アドバ...  \n",
       "\n",
       "[8062 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
